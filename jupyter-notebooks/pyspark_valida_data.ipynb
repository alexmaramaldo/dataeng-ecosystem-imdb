{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0050c8-6ade-42f8-a9c8-f9194de69478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from datetime import datetime\n",
    "from decouple import config\n",
    "from datetime import datetime\n",
    "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aabb4b7-85ba-4d5f-b070-2a8c23c35f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-java-sdk-1.12.400.jar\t    httpclient5-5.2.1.jar\n",
      "aws-java-sdk-bundle-1.12.231.jar    jets3t-0.9.4.jar\n",
      "aws-java-sdk-core-1.12.400.jar\t    joda-time-2.12.2.jar\n",
      "aws-java-sdk-dynamodb-1.12.400.jar  postgresql-42.6.0.jar\n",
      "aws-java-sdk-kms-1.12.400.jar\t    slf4j-api-2.0.6.jar\n",
      "aws-java-sdk-s3-1.12.400.jar\t    slf4j-reload4j-2.0.6.jar\n",
      "hadoop-aws-3.3.1.jar\n"
     ]
    }
   ],
   "source": [
    "mypath = \"/opt/spark_jars\"\n",
    "files = glob.glob(f'{mypath}/*')\n",
    "my_jars = ','.join(files)\n",
    "aws_access_key = config('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_key = config('AWS_SECRET_ACCESS_KEY')\n",
    "aws_endpoint_url = \"http://dataeng-minio-nginx:9000\"\n",
    "\n",
    "current_time = datetime.now()\n",
    "current_date = current_time .strftime(\"%Y-%m-%d\")\n",
    "year = current_time .strftime(\"%Y\")\n",
    "month = current_time .strftime(\"%m\")\n",
    "day = current_time .strftime(\"%d\")\n",
    "current_folder = f\"year={year}/month={month}/day={day}\"\n",
    "\n",
    "!ls /opt/spark_jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36006e9-0385-44f7-93b2-84065880caba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/20 19:42:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# spark.stop()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"JupyterLabDataExtraction\") \\\n",
    "    .config(\"spark.jars\", my_jars) \\\n",
    "    .master(config('SPARK_URL')) \\\n",
    "    .getOrCreate() \n",
    "\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", aws_endpoint_url)\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", aws_access_key)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9386d4d-38ef-4915-9261-aad720e7325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading title_ratings\n",
    "\n",
    "foldername = f\"title_ratings/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_rating_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "title_rating_raw_df.createOrReplaceTempView(\"title_ratings\")\n",
    "# title_rating_raw_df.printSchema(), title_rating_raw_df.count(), title_rating_raw_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8616930-88c5-43f1-9d58-022663f5992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading title_akas: responsavel pelo o nome dos titulos por regi√£o e idioma\n",
    "\n",
    "foldername = f\"title_akas/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_akas_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "# title_akas_raw_df.printSchema(), title_akas_raw_df.count(), title_akas_raw_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720151e0-37d3-46f7-8f0c-ea6cab2f595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering by country, in this case: BR\n",
    "title_akas_br_raw_df = title_akas_raw_df.where(title_akas_raw_df.region == \"BR\")\n",
    "title_akas_br_raw_df.createOrReplaceTempView(\"title_akas_br\")\n",
    "# title_akas_br_raw_df.count(), title_akas_br_raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b3d5d-33d9-4722-b0e0-11dc09f56c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        title_akas_br ta\n",
    "    left join\n",
    "        title_ratings tr on ta.titleId = tr.tconst\n",
    "\"\"\"\n",
    "akas_and_rating_df = spark.sql(sql);\n",
    "\n",
    "akas_and_rating_df.createOrReplaceTempView(\"titles_akas_ratings_br\")\n",
    "\n",
    "# akas_and_rating_df.count(), akas_and_rating_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096efd8-5cd1-4bc6-939e-5c37213d9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_more_voteds = \"\"\"\n",
    "    select\n",
    "        tr.title,\n",
    "        tr.numVotes\n",
    "    from\n",
    "        titles_akas_ratings_br tr\n",
    "    order by \n",
    "        tr.numVotes DESC\n",
    "    limit 10\n",
    "    \n",
    "\"\"\"\n",
    "more_votes_df = spark.sql(sql_more_voteds).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46422e88-de9f-4be3-8ec3-da991264f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_votes_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee123fa-0d46-4537-8ecb-a88042ef56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_more_voteds_pd_df = akas_and_rating_df.select('title', 'numVotes').sort(akas_and_rating_df.numVotes.desc(), akas_and_rating_df.averageRating.desc()).limit(10).toPandas()\n",
    "# py_more_voteds_pd_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c508a26-cc70-4345-b6e5-5744ddb9228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(py_more_voteds_pd_df.title,py_more_voteds_pd_df.numVotes)\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10655914-6118-452a-8f49-1609a060434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading title_basics\n",
    "\n",
    "foldername = f\"title_basics/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_basics_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "# title_basics_raw_df.createOrReplaceTempView(\"title_basics\")\n",
    "# title_basics_raw_df.printSchema(), title_basics_raw_df.count(), title_basics_raw_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed89bce-b25b-4c1f-a7e9-3abba44e800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_title_basics_row_df = title_basics_raw_df.sort(title_basics_raw_df.tconst.desc()).limit(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083a203-32d5-4608-a91a-63208f6e206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_title_basics_row_df.write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://dataeng-postgres:5432/awari_imdb\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"title_basics\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"postgres\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279e87e-4ec1-49e0-99bc-6f3d8c239763",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ec9ae-c74b-44e6-9414-2ab6c3e8ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading name_basics: Name from Directos, Actor, everybody from casting, etc\n",
    "# root\n",
    "#  |-- nconst: string (nullable = true)\n",
    "#  |-- primaryName: string (nullable = true)\n",
    "#  |-- birthYear: string (nullable = true)\n",
    "#  |-- deathYear: string (nullable = true)\n",
    "#  |-- primaryProfession: string (nullable = true)\n",
    "#  |-- knownForTitles: string (nullable = true)\n",
    "# count: ~12.685.224 registers\n",
    "\n",
    "module = \"name_basics\"\n",
    "\n",
    "foldername = f\"{module}/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "name_basics_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "name_basics_raw_df.createOrReplaceTempView(\"name_basics\")\n",
    "# name_basics_raw_df.printSchema(), name_basics_raw_df.count(), name_basics_raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634514bf-0eb1-4d63-8425-c5719e254e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading title_akas: Catalog with Titles Names in another languages\n",
    "# root\n",
    "#  |-- titleId: string (nullable = true)\n",
    "#  |-- ordering: string (nullable = true)\n",
    "#  |-- title: string (nullable = true)\n",
    "#  |-- region: string (nullable = true)\n",
    "#  |-- language: string (nullable = true)\n",
    "#  |-- types: string (nullable = true)\n",
    "#  |-- attributes: string (nullable = true)\n",
    "#  |-- isOriginalTitle: string (nullable = true)\n",
    "# count: ~36.602.195 registers\n",
    "\n",
    "module = \"title_akas\"\n",
    "\n",
    "foldername = f\"{module}/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_akas_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "title_akas_raw_df.createOrReplaceTempView(\"title_akas\")\n",
    "# title_akas_raw_df.printSchema(), title_akas_raw_df.count(), title_akas_raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f009eef-eeff-4c9a-8502-e3c56b4054b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading title_basics: All Titles with bascis fields\n",
    "# root\n",
    "#  |-- tconst: string (nullable = true)\n",
    "#  |-- titleType: string (nullable = true)\n",
    "#  |-- primaryTitle: string (nullable = true)\n",
    "#  |-- originalTitle: string (nullable = true)\n",
    "#  |-- isAdult: string (nullable = true)\n",
    "#  |-- startYear: string (nullable = true)\n",
    "#  |-- endYear: string (nullable = true)\n",
    "#  |-- runtimeMinutes: string (nullable = true)\n",
    "#  |-- genres: string (nullable = true)\n",
    "# count: ~10.006.917 registers\n",
    "\n",
    "module = \"title_basics\"\n",
    "\n",
    "foldername = f\"{module}/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_basics_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "title_basics_raw_df.createOrReplaceTempView(\"title_basics\")\n",
    "# title_basics_raw_df.printSchema(), title_basics_raw_df.count(), title_basics_raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6509a-12b9-4259-9662-4bd33b807d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading title_crew: Relationship between Names Basics and Titles\n",
    "# root\n",
    "#  |-- tconst: string (nullable = true)\n",
    "#  |-- directors: string (nullable = true)\n",
    "#  |-- writers: string (nullable = true)\n",
    "# count: ~10.006.917 registers\n",
    "\n",
    "module = \"title_crew\"\n",
    "\n",
    "foldername = f\"{module}/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_crew_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "title_crew_raw_df.createOrReplaceTempView(\"title_crew\")\n",
    "# title_crew_raw_df.printSchema(), title_crew_raw_df.count(), title_crew_raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3630d-65e3-45b4-bca9-0193c111fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading title_episode: Relationships with all title that are related for a Title, like a SHOW(Series em portugues), and seasons\n",
    "# root\n",
    "#  |-- tconst: string (nullable = true)\n",
    "#  |-- parentTconst: string (nullable = true)\n",
    "#  |-- seasonNumber: string (nullable = true)\n",
    "#  |-- episodeNumber: string (nullable = true)\n",
    "# count: ~7.612.350 registers\n",
    "\n",
    "module = \"title_episode\"\n",
    "\n",
    "foldername = f\"{module}/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_episode_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "title_episode_raw_df.createOrReplaceTempView(\"title_episode\")\n",
    "# title_episode_raw_df.printSchema(), title_episode_raw_df.count(), title_episode_raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27704bb-ba96-45ca-a3c5-8743e835ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading title_principals: Name of main Names related to Title, like : Director, Main Actor, etc\n",
    "# root\n",
    "#  |-- tconst: string (nullable = true)\n",
    "#  |-- ordering: string (nullable = true)\n",
    "#  |-- nconst: string (nullable = true)\n",
    "#  |-- category: string (nullable = true)\n",
    "#  |-- job: string (nullable = true)\n",
    "#  |-- characters: string (nullable = true)\n",
    "# count: ~57.165.969 registers\n",
    "\n",
    "\n",
    "module = \"title_principals\"\n",
    "\n",
    "foldername = f\"{module}/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_principals_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "title_principals_raw_df.createOrReplaceTempView(\"title_principals\")\n",
    "# title_principals_raw_df.printSchema(), title_principals_raw_df.count(), title_principals_raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "533070dd-e945-4c28-bb35-139e612245da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/20 19:42:54 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- averageRating: string (nullable = true)\n",
      " |-- numVotes: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    1987|\n",
      "|tt0000002|          5.8|     265|\n",
      "+---------+-------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 1331252, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading title_ratings\n",
    "# root\n",
    "#  |-- tconst: string (nullable = true)\n",
    "#  |-- averageRating: string (nullable = true)\n",
    "#  |-- numVotes: string (nullable = true)\n",
    "# count: ~1.329.800 registers\n",
    "\n",
    "module = \"title_ratings\"\n",
    "\n",
    "foldername = f\"{module}/{current_folder}\"\n",
    "fileurl = f\"s3a://{os.getenv('AWS_S3_IMDB_BUCKET')}/datalake/{foldername}/data\"\n",
    "title_ratings_raw_df = spark.read.parquet(fileurl, sep=r'\\t', header=True)\n",
    "title_ratings_raw_df.createOrReplaceTempView(\"title_ratings\")\n",
    "# title_ratings_raw_df.printSchema(), title_ratings_raw_df.count(), title_ratings_raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef216b0b-ef1f-4646-8bba-3465b744f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "modules = ['name_basics', 'title_akas', 'title_basics', 'title_crew', 'title_episode', 'title_principals', 'title_ratings']\n",
    "totals = [\n",
    "    name_basics_raw_df.count(), \n",
    "    title_akas_raw_df.count(), \n",
    "    title_basics_raw_df.count(), \n",
    "    title_crew_raw_df.count(), \n",
    "    title_episode_raw_df.count(), \n",
    "    title_principals_raw_df.count(), \n",
    "    title_ratings_raw_df.count()\n",
    "]\n",
    "\n",
    "# get the length of the longest string in text, for the numpy str dtype\n",
    "# this is only necessary if make sure the entire string is included in the array\n",
    "str_len = max([len(t) for t in modules])\n",
    "\n",
    "# create numpy array with dtypes\n",
    "t = np.array(list(zip(modules, totals)), dtype = [('text', f'S{str_len}'), ('values', int)])\n",
    "\n",
    "# sort array\n",
    "t = np.sort(t, order=['values'])[::-1]\n",
    "\n",
    "# print(np.sort(t['values']))\n",
    "\n",
    "ax.bar(x=t['text'], height=t['values'])\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "# plt.gca().set_yticklabels(['{:,.0f}'.format(x) for x in np.sort(t['values'])])\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5670f90-a1de-414e-bb6d-eeeb52e42330",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec005f4c-46cb-4d3d-9204-9078910e001d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
